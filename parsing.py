import os
import time

import requests
from bs4 import BeautifulSoup

BASE_URL = "https://www.president.gov.ua"
ORIGINAL_URL = BASE_URL + "/news/speeches"
WAYBACK_API = "https://archive.org/wayback/available"
SAVE_DIR = "speeches_texts"
os.makedirs(SAVE_DIR, exist_ok=True)


def get_latest_snapshot_url(target_url, retries=5, delay=3):
    """
    –ü–æ–ª—É—á–∞–µ—Ç URL –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–Ω–∞–ø—à–æ—Ç–∞ –∏–∑ Wayback Machine.
    –î–µ–ª–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—ã—Ç–æ–∫, –µ—Å–ª–∏ —Å–Ω–∞–ø—à–æ—Ç –µ—â—ë –Ω–µ –ø–æ—è–≤–∏–ª—Å—è.

    :param target_url: –∞–¥—Ä–µ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    :param retries: —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø—Ä–æ–±–æ–≤–∞—Ç—å
    :param delay: –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ (—Å–µ–∫)
    :return: url —Å–Ω–∞–ø—à–æ—Ç–∞ –∏–ª–∏ None
    """
    params = {"url": target_url}

    for attempt in range(1, retries + 1):
        res = requests.get(WAYBACK_API, params=params)
        data = res.json()
        print(f"[{attempt}/{retries}] –û—Ç–≤–µ—Ç API:", data)

        try:
            return data["archived_snapshots"]["closest"]["url"]
        except KeyError:
            print("‚è≥ –°–Ω–∞–ø—à–æ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∂–¥—ë–º...")
            time.sleep(delay)

    print("‚ùå Snapshot —Ç–∞–∫ –∏ –Ω–µ –ø–æ—è–≤–∏–ª—Å—è.")
    return None

def get_article_links_from_snapshot(snapshot_url):
    print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∞—Ä—Ö–∏–≤–∞: {snapshot_url}")
    res = requests.get(snapshot_url)
    soup = BeautifulSoup(res.text, "html.parser")

    links = []
    for a in soup.select("div.item_stat.cat_stat h3 a[href]"):
        href = a["href"]
        if not href.startswith("http"):
            href = BASE_URL + href
        links.append(href)

    return links

def extract_filename_from_url(url):
    return url.split("/")[-1].split("?")[0] + ".txt"

def parse_speech(url):
    try:
        filename = extract_filename_from_url(url)
        filepath = os.path.join(SAVE_DIR, filename)

        if os.path.exists(filepath):
            print(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞—é (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç): {filename}")
            return

        print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞—é: {url}")
        res = requests.get(url)
        soup = BeautifulSoup(res.text, "html.parser")

        article_div = soup.find("div", class_="article_content", itemprop="articleBody")
        if not article_div:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –±–ª–æ–∫–∞.")
            return

        paragraphs = article_div.find_all("p")
        full_text = "\n\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(full_text)

        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {filepath}")
    except Exception as e:
        print(f"‚ùó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")


if __name__ == "__main__":
    snapshot_url = get_latest_snapshot_url(ORIGINAL_URL)
    if snapshot_url:
        links = get_article_links_from_snapshot(snapshot_url)
        print(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫.")
        for link in links:
            parse_speech(link)
